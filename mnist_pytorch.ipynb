{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mnist_pytorch.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "6QA1D6eIszX_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 0: Initial setup\n",
        "\n",
        "To enable GPU:\n",
        "1.   Click Edit -> Notebook settings\n",
        "2.   Under Hardware Accelerator select GPU\n",
        "3.   On the right side of this page, click connect to a hosted runtime\n",
        "\n",
        "\n",
        "If you ever see an error about needing third-party cookies enabled, you can disable blocking them or whitelist them.\n",
        "Here is a simple way to whitelist (in chrome)\n",
        "\n",
        "For old chrome:\n",
        "1.   Goto chrome://settings/content/cookies>search=cookie\n",
        "2.   Uncheck \"Block third-party cookies\" or\n",
        "3.   Click Add next to Allow and type https://[*.]googleusercontent.com:443\n",
        "\n",
        "For new chrome:\n",
        "1.   Goto settings and search \"cookie\"\n",
        "2.   Click the \"content settings\" button\n",
        "3.   Follow steps 2 or 3 from above\n",
        "---\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "DHxEgxT6YwR8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "43e2b038-cc6e-429b-bc59-273a9d755dc3"
      },
      "cell_type": "code",
      "source": [
        "# This shows how to connect your google drive account with a colab instance. It's pretty easy.\n",
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/gdrive')\n",
        "# Create a directory and mount Google Drive using that directory.\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Gv3f_-svjwm6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Drive not connecting after it seemed like it worked before?\n",
        "1. First try restarting the runtime via Runtime -> Restart Runtime\n",
        "2. Then try to run the above again.\n",
        "3. If this still doesn't work, call Reset All Runtimes. This is the nuclear option that will delete all your data not saved on your personal drive account, and will erase everything you installed.\n"
      ]
    },
    {
      "metadata": {
        "id": "Ae5vt-EyTl-9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1751
        },
        "outputId": "80cab7ef-43bb-41bb-a27d-d46ca07b22da"
      },
      "cell_type": "code",
      "source": [
        "# Now let's test that Google Drive is up and running. \n",
        "# You may have to change \"My Drive\" if you have renamed it something else.\n",
        "!ls \"/gdrive/My Drive\"\n",
        "\n",
        "with open('/gdrive/My Drive/foo.txt', 'w') as f:\n",
        "  f.write('Hello Google Drive!')\n",
        "!cat \"/gdrive/My Drive/foo.txt\"\n",
        "!rm \"/gdrive/My Drive/foo.txt\""
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'2nd Sem Final Study Guide English.gdoc'\n",
            "'421 HW 6.gdoc'\n",
            "'490D Discussion Questions.gdoc'\n",
            "'ACAD 198 Project.gdoc'\n",
            " Akshat\n",
            " AkshatResumeOct20.pdf\n",
            "'Amy Feedback.gdoc'\n",
            "\"Amy's Feedback on Phil Paper.gdoc\"\n",
            "'AP Biology Chapter 25-27.gdoc'\n",
            "'AP Biology Exam Review-Key.docx'\n",
            "'AP Biology Final Study Guide.gdoc'\n",
            "'AP Biology FRQ Practice.gdoc'\n",
            "'AP Biology Lab pGLO.gdoc'\n",
            "'AP Biology Notes.gdoc'\n",
            "'AP Biology Presentation .gdoc'\n",
            "'AP Biology Video Questions.gdoc'\n",
            "'APES Unit 6 Notebook Stuff.gdoc'\n",
            "'APES Unit 7: Sidebars.gdoc'\n",
            "'APES Unit 7 Study Guide.gdoc'\n",
            "'APES Unit 9 Notebook.gdoc'\n",
            "'AP Lab 12.gdoc'\n",
            "'AP Lab Invert.gdoc'\n",
            "'Aquifer Education Plan.gdoc'\n",
            "'Aquifer Presentation.gslides'\n",
            "'bomba de vacio para pene.gdoc'\n",
            " Brochure.gdoc\n",
            "'Chem Review.gdoc'\n",
            " colab_files\n",
            "'Colab Notebooks'\n",
            "'Copy of Trying to Get Rush Tickets.gsheet'\n",
            "'CSE 333.gdoc'\n",
            " CSE344\n",
            " CSE401_2_1c.html\n",
            "'CSE 444 HW 3.gdoc'\n",
            "'CSE 444 HW 4.gdoc'\n",
            "'CSE 444 HW 6.gdoc'\n",
            "'CSE 446 HW 2.gdoc'\n",
            "'CSE 447 HW 4.gdoc'\n",
            "'CSE 447 NLP HW 1.gdoc'\n",
            "'CSE 447 NLP HW 2.gdoc'\n",
            "'CSE 447 Notes.gdoc'\n",
            "'CSE Personal Statement.gdoc'\n",
            "'Digital Inclusion and the Role of Mobile in Nigeria (CSE 490D).gslides'\n",
            "'Engl 266 Midterm.gdoc'\n",
            " Extracirricular\n",
            "'Final Exam ENGL 266.gdoc'\n",
            "'Final Version Phil Paper.gdoc'\n",
            " Grabiner-The_Origins_of_Cauchys_Rigorous_Calculus_2005.djvu\n",
            "'Graph Questions.gdoc'\n",
            " hw2.zip\n",
            " HW3.gdoc\n",
            "'La Pirateria.gdoc'\n",
            "'Masters Project.gdoc'\n",
            "'Math Project.pdf'\n",
            "'MBTI and SII Reflection.gdoc'\n",
            " Music.gdoc\n",
            " phil\n",
            "'PHIL 102'\n",
            "'Phil 102 Final Paper.gdoc'\n",
            "'PHIL 102 First Paper.gdoc'\n",
            "'PHIL 102 - Second Paper.gdoc'\n",
            "'Phil 120 Final Vocab.gdoc'\n",
            " Project.gdoc\n",
            "'PSYCH 101.gdoc'\n",
            "'Psych FInal Exam.gdoc'\n",
            "'Psych Midterm 2.gdoc'\n",
            "'Psych Midterm 3.gdoc'\n",
            "'quiz 1.gdoc'\n",
            " Russell-Introduction_to_Mathematical_Philosophy.djvu\n",
            " Schizophrenia.gdoc\n",
            " School\n",
            "'September 29th.gdoc'\n",
            " ShortestClickDistance-DevelopmentPlan.gdoc\n",
            " Skydive\n",
            "'Spanish Pineapple Presentation .gdoc'\n",
            "'Spanish Written Portion.gdoc'\n",
            "'Starting Knowledge.gdoc'\n",
            "'Summer 2015.gdoc'\n",
            "'Swag Project.gdoc'\n",
            "'TYE Notes.gdoc'\n",
            "'TYE Presentation.gslides'\n",
            "'Unit 9 APES Study Guide.gdoc'\n",
            "'Untitled Diagram (1).html'\n",
            "'Untitled Diagram.html'\n",
            "'Untitled document (1).gdoc'\n",
            "'Untitled document (2).gdoc'\n",
            "'Untitled document (3).gdoc'\n",
            "'Untitled document (4).gdoc'\n",
            "'Untitled document (5).gdoc'\n",
            "'Untitled document (6).gdoc'\n",
            "'Untitled document.gdoc'\n",
            "'Untitled form.gform'\n",
            "'Untitled presentation.gslides'\n",
            "'UW essay Notes.gdoc'\n",
            "'UW List of Things to Do.gdoc'\n",
            "'UW Unofficial Academic Transcript.pdf'\n",
            "'WA State Licensing: Get your first driver license (1).pdf'\n",
            "'WA State Licensing: Get your first driver license.pdf'\n",
            "'Wikipedia Shortest Click Project.gdoc'\n",
            "'Window Transform Proposal.gdoc'\n",
            "'Works Consulted.gdoc'\n",
            "Hello Google Drive!"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oINm7sOOdpaL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "11fe31ee-aeb8-4cde-e6ee-ab99f9cd69fe"
      },
      "cell_type": "code",
      "source": [
        "# This is code to download and install pytorch\n",
        "from os import path\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "\n",
        "accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
        "  \n",
        "import torch\n",
        "print('Version', torch.__version__)\n",
        "print('CUDA enabled:', torch.cuda.is_available())\n",
        "  \n",
        "# NOTE: This one takes a while the first time you run it, and you will likely see \n",
        "# tcmalloc: large alloc 1073750016 bytes == 0x5c54a000 @ or something similar.\n",
        "# It should then print out:\n",
        "# Version 0.4.1\n",
        "# CUDA enabled: True"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Version 0.4.1\n",
            "CUDA enabled: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lrYXOGpsM6TV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define some useful save and restoring functions. \n",
        "# You can thank your TAs for providing this code, \n",
        "# it will probably be useful for you in the future as well.\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import os\n",
        "class pt_util(object):\n",
        "    @staticmethod\n",
        "    # This does more than the simple Pytorch restore. It checks that the names \n",
        "    # of variables match, and if they don't doesn't throw a fit. It is similar \n",
        "    # to how Caffe acts. This is especially useful if you decide to change your\n",
        "    # network architecture but don't want to retrain from scratch.\n",
        "    def restore(net, save_file):\n",
        "        net_state_dict = net.state_dict()\n",
        "        restore_state_dict = torch.load(save_file)\n",
        "\n",
        "        restored_var_names = set()\n",
        "\n",
        "        print('Restoring:')\n",
        "        for var_name in restore_state_dict.keys():\n",
        "            if var_name in net_state_dict:\n",
        "                var_size = net_state_dict[var_name].size()\n",
        "                restore_size = restore_state_dict[var_name].size()\n",
        "                if var_size != restore_size:\n",
        "                    print('Shape mismatch for var', var_name, 'expected', var_size, 'got', restore_size)\n",
        "                else:\n",
        "                    if isinstance(net_state_dict[var_name], torch.nn.Parameter):\n",
        "                        # backwards compatibility for serialized parameters\n",
        "                        net_state_dict[var_name] = restore_state_dict[var_name].data\n",
        "                    try:\n",
        "                        net_state_dict[var_name].copy_(restore_state_dict[var_name])\n",
        "                        print(str(var_name) + ' -> \\t' + str(var_size) + ' = ' + str(int(np.prod(var_size) * 4 / 10**6)) + 'MB')\n",
        "                        restored_var_names.add(var_name)\n",
        "                    except:\n",
        "                        print('While copying the parameter named {}, whose dimensions in the model are'\n",
        "                              ' {} and whose dimensions in the checkpoint are {}, ...'.format(\n",
        "                                  var_name, var_size, restore_size))\n",
        "                        raise\n",
        "\n",
        "        ignored_var_names = sorted(list(set(restore_state_dict.keys()) - restored_var_names))\n",
        "        unset_var_names = sorted(list(set(net_state_dict.keys()) - restored_var_names))\n",
        "        print('')\n",
        "        if len(ignored_var_names) == 0:\n",
        "            print('Restored all variables')\n",
        "        else:\n",
        "            print('Did not restore:\\n\\t' + '\\n\\t'.join(ignored_var_names))\n",
        "        if len(unset_var_names) == 0:\n",
        "            print('No new variables')\n",
        "        else:\n",
        "            print('Initialized but did not modify:\\n\\t' + '\\n\\t'.join(unset_var_names))\n",
        "\n",
        "        print('Restored %s' % save_file)\n",
        "        \n",
        "    @staticmethod\n",
        "    def restore_latest(net, folder):\n",
        "        import glob\n",
        "        import re\n",
        "        checkpoints = sorted(glob.glob(folder + '/*.pt'), key=os.path.getmtime)\n",
        "        start_it = 0\n",
        "        if len(checkpoints) > 0:\n",
        "            pt_util.restore(net, checkpoints[-1])\n",
        "            start_it = int(re.findall(r'\\d+', checkpoints[-1])[-1])\n",
        "        return start_it\n",
        "\n",
        "    @staticmethod\n",
        "    def save(net, file_name, num_to_keep=1):\n",
        "        folder = os.path.dirname(file_name)\n",
        "        if not os.path.exists(folder):\n",
        "            os.makedirs(folder)\n",
        "        torch.save(net.state_dict(), file_name)\n",
        "        import glob\n",
        "        extension = os.path.splitext(file_name)[1]\n",
        "        checkpoints = sorted(glob.glob(folder + '/*' + extension), key=os.path.getmtime)\n",
        "        print('Saved %s\\n' % file_name)\n",
        "        if num_to_keep > 0:\n",
        "            for ff in checkpoints[:-num_to_keep]:\n",
        "                os.remove(ff)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u486U-pUJnDY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 1: Implementing a network for MNIST"
      ]
    },
    {
      "metadata": {
        "id": "0oKKR9EsoK9G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# This is where you define your network architecture.\n",
        "# Note: The TAs know this follows the PyTorch MNIST tutorial available at \n",
        "# https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
        "# Where do you think we got it from? \n",
        "# So we are asking you to implement something slightly different. \n",
        "# You can use that as a guide, but make sure you understand what it all does.\n",
        "\n",
        "\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "class MNISTNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MNISTNet, self).__init__()\n",
        "        # The network should be as follows:\n",
        "        # One fully connected layer with 1024 outputs.\n",
        "        # One fully connected layer with 512 outputs.\n",
        "        # Then the final classification layer.\n",
        "        # All the nonlinearities should be ReLU.\n",
        "        # These instructions are vague on purpose.\n",
        "        \n",
        "        # input (m x 784)\n",
        "        self.fc1 = nn.Linear(784, 1024)\n",
        "        self.fc2 = nn.Linear(1024, 512)\n",
        "        self.fc3 = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = x.view(-1, 28*28)\n",
        "      x = F.relu(self.fc1(x))\n",
        "      x = F.relu(self.fc2(x))\n",
        "      x = self.fc3(x)\n",
        "      return x\n",
        "      \n",
        "    def save_model(self, file_path, num_to_keep=1):\n",
        "        pt_util.save(self, file_path, num_to_keep)\n",
        "        \n",
        "    def loss(self, prediction, label, reduction='elementwise_mean'):\n",
        "        # You should also use the cross_entropy loss rather than the NLL loss.\n",
        "        return F.cross_entropy(prediction, label, reduction=reduction)\n",
        "\n",
        "\n",
        "    def load_model(self, file_path):\n",
        "        pt_util.restore(self, file_path)\n",
        "\n",
        "    def load_last_model(self, dir_path):\n",
        "        return pt_util.restore_latest(self, dir_path)\n",
        "\n",
        "\n",
        "def train(model, device, train_loader, optimizer, epoch, log_interval):\n",
        "    model.train()\n",
        "    for batch_idx, (data, label) in enumerate(train_loader):\n",
        "        data, label = data.to(device), label.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data)\n",
        "        loss = model.loss(out, label)\n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, label in test_loader:\n",
        "            data, label = data.to(device), label.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += model.loss(output, label)\n",
        "            pred = output.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(label.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pNf3AoHVvKXI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Play around with these constants, you may find a better setting.\n",
        "BATCH_SIZE = 256\n",
        "TEST_BATCH_SIZE = 1000\n",
        "EPOCHS = 10\n",
        "LEARNING_RATE = 0.01\n",
        "MOMENTUM = 0.5\n",
        "USE_CUDA = True\n",
        "SEED = 0\n",
        "LOG_INTERVAL = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "50zDbqjXu_Qq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "04ac5d1f-7ab9-48bb-f273-f6592ce82f6e"
      },
      "cell_type": "code",
      "source": [
        "# Now the actual training code\n",
        "use_cuda = USE_CUDA and torch.cuda.is_available()\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print('Using device', device)\n",
        "import multiprocessing\n",
        "import torch.utils\n",
        "import torch.utils.data\n",
        "from torchvision import datasets, transforms\n",
        "print('num cpus:', multiprocessing.cpu_count())\n",
        "\n",
        "kwargs = {'num_workers': multiprocessing.cpu_count(),\n",
        "          'pin_memory': True} if use_cuda else {}\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ])),\n",
        "    batch_size=BATCH_SIZE, shuffle=True, **kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ])),\n",
        "    batch_size=TEST_BATCH_SIZE, **kwargs)\n",
        "\n",
        "\n",
        "model = MNISTNet().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)\n",
        "# This will save checkpoints in your Google Drive account.\n",
        "start_epoch = model.load_last_model('/gdrive/My Drive/colab_files/homework1/mnist/checkpoints')\n",
        "try:\n",
        "    for epoch in range(start_epoch, EPOCHS + 1):\n",
        "        train(model, device, train_loader, optimizer, epoch, LOG_INTERVAL)\n",
        "        test(model, device, test_loader)\n",
        "        model.save_model('/gdrive/My Drive/colab_files/homework1/mnist/checkpoints/%03d.pt' % epoch)\n",
        "except KeyboardInterrupt as ke:\n",
        "    print('Interrupted')\n",
        "except:\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "finally:\n",
        "    model.save_model('/gdrive/My Drive/colab_files/homework1/mnist/checkpoints/%03d.pt' % epoch)\n",
        "        "
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using device cuda\n",
            "num cpus: 2\n",
            "Restoring:\n",
            "fc1.weight -> \ttorch.Size([1024, 784]) = 3MB\n",
            "fc1.bias -> \ttorch.Size([1024]) = 0MB\n",
            "fc2.weight -> \ttorch.Size([512, 1024]) = 2MB\n",
            "fc2.bias -> \ttorch.Size([512]) = 0MB\n",
            "fc3.weight -> \ttorch.Size([10, 512]) = 0MB\n",
            "fc3.bias -> \ttorch.Size([10]) = 0MB\n",
            "\n",
            "Restored all variables\n",
            "No new variables\n",
            "Restored /gdrive/My Drive/colab_files/homework1/mnist/checkpoints/010.pt\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.095639\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.152777\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.106086\n",
            "\n",
            "Test set: Average loss: 0.0001, Accuracy: 9620/10000 (96%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework1/mnist/checkpoints/010.pt\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework1/mnist/checkpoints/010.pt\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bBHdoWQZkaZD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 2: Reimplementing the Cross Entropy loss function."
      ]
    },
    {
      "metadata": {
        "id": "wg8_YkUxk1QB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MNISTNetNewLoss(MNISTNet):\n",
        "  \n",
        "    def loss(self, prediction, label, reduction='elementwise_mean'): \n",
        "        # Reimplement the Cross Entropy loss function using mathematical primitives.\n",
        "        # This means you are not allowed to use any function in the \"Loss functions\" \n",
        "        # section of https://pytorch.org/docs/stable/nn.html#id50 nor the \n",
        "        # functional versions. You can use them to verify that your output looks correct.\n",
        "        # You should implement reduction for none (i.e. return a vector, sum, and elementwise_mean).\n",
        "        # Note: Due to floating point errors, the values won't be exactly equal.\n",
        "        # Second note: You can assume inputs will be 2D (batch X features).\n",
        "        # print('test', prediction) # (256, 10) \n",
        "        loss_val_old = super(MNISTNetNewLoss, self).loss(prediction, label, reduction) \n",
        "        \n",
        "        ft = prediction[torch.arange(prediction.shape[0]), label]\n",
        "        exp_prediction = torch.exp(prediction)\n",
        "        st = torch.log(torch.sum(exp_prediction, 1))\n",
        "        loss_val_new = -ft + st\n",
        "        \n",
        "        \n",
        "        if reduction == \"elementwise_mean\":\n",
        "          loss_val_new = torch.mean(loss_val_new)\n",
        "        elif reduction == \"sum\":\n",
        "          loss_val_new = torch.sum(loss_val_new)\n",
        "        \n",
        "        assert(abs(loss_val_new - loss_val_old).item() < 0.01)\n",
        "        \n",
        "        return loss_val_new\n",
        "\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MIKHYa71mD79",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1581
        },
        "outputId": "a1d2568c-6fa0-4598-f0fb-3ac8eed34109"
      },
      "cell_type": "code",
      "source": [
        "# Now the actual training code\n",
        "use_cuda = USE_CUDA and torch.cuda.is_available()\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print('Using device', device)\n",
        "import multiprocessing\n",
        "print('num cpus:', multiprocessing.cpu_count())\n",
        "\n",
        "kwargs = {'num_workers': multiprocessing.cpu_count(),\n",
        "          'pin_memory': True} if use_cuda else {}\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ])),\n",
        "    batch_size=BATCH_SIZE, shuffle=True, **kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ])),\n",
        "    batch_size=TEST_BATCH_SIZE, **kwargs)\n",
        "\n",
        "\n",
        "model = MNISTNetNewLoss().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)\n",
        "start_epoch = model.load_last_model('/gdrive/My Drive/colab_files/homework1/mnist2/checkpoints')\n",
        "try:\n",
        "    for epoch in range(start_epoch, EPOCHS + 1):\n",
        "        train(model, device, train_loader, optimizer, epoch, LOG_INTERVAL)\n",
        "        test(model, device, test_loader)\n",
        "        model.save_model('/gdrive/My Drive/colab_files/homework1/mnist2/checkpoints/%03d.pt' % epoch)\n",
        "except KeyboardInterrupt as ke:\n",
        "    print('Interrupted')\n",
        "except:\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "finally:\n",
        "    model.save_model('/gdrive/My Drive/colab_files/homework1/mnist2/checkpoints/%03d.pt' % epoch)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using device cuda\n",
            "num cpus: 2\n",
            "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.303680\n",
            "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.888741\n",
            "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.494680\n",
            "\n",
            "Test set: Average loss: 0.0004, Accuracy: 8907/10000 (89%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework1/mnist2/checkpoints/000.pt\n",
            "\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.365670\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.379490\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.348167\n",
            "\n",
            "Test set: Average loss: 0.0003, Accuracy: 9130/10000 (91%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework1/mnist2/checkpoints/001.pt\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.306437\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.339680\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.267435\n",
            "\n",
            "Test set: Average loss: 0.0003, Accuracy: 9228/10000 (92%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework1/mnist2/checkpoints/002.pt\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.248540\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.301812\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.239199\n",
            "\n",
            "Test set: Average loss: 0.0002, Accuracy: 9291/10000 (93%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework1/mnist2/checkpoints/003.pt\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.289475\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.221326\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.272655\n",
            "\n",
            "Test set: Average loss: 0.0002, Accuracy: 9367/10000 (94%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework1/mnist2/checkpoints/004.pt\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.227357\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.210724\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.255594\n",
            "\n",
            "Test set: Average loss: 0.0002, Accuracy: 9413/10000 (94%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework1/mnist2/checkpoints/005.pt\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.296861\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.298260\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.192634\n",
            "\n",
            "Test set: Average loss: 0.0002, Accuracy: 9462/10000 (95%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework1/mnist2/checkpoints/006.pt\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.182246\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.171570\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.191295\n",
            "\n",
            "Test set: Average loss: 0.0002, Accuracy: 9500/10000 (95%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework1/mnist2/checkpoints/007.pt\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.188739\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.134154\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.166723\n",
            "\n",
            "Test set: Average loss: 0.0002, Accuracy: 9514/10000 (95%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework1/mnist2/checkpoints/008.pt\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.165912\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.146911\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.114854\n",
            "\n",
            "Test set: Average loss: 0.0002, Accuracy: 9553/10000 (96%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework1/mnist2/checkpoints/009.pt\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.121347\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.145681\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.111495\n",
            "\n",
            "Test set: Average loss: 0.0001, Accuracy: 9561/10000 (96%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework1/mnist2/checkpoints/010.pt\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework1/mnist2/checkpoints/010.pt\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}